import json
import ast
from langchain_openai import ChatOpenAI
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.chains import ConversationChain, LLMChain
from langchain_core.prompts import ChatPromptTemplate
from langchain.prompts import (
    ChatPromptTemplate, HumanMessagePromptTemplate,
    MessagesPlaceholder, PromptTemplate
)
from langchain.memory import ConversationBufferMemory
# from tools.rag_tool import vectorstore
from agent import create_agent

# â–¶ï¸ GPT ëª¨ë¸ ì´ˆê¸°í™”
chat_model = ChatOpenAI(
    temperature=0.7,
    model_name="gpt-4o",
)

# â–¶ï¸ ëŒ€í™” ë©”ëª¨ë¦¬
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
# retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

# â–¶ï¸ ì´ˆê¸° ì‚¬ìš©ì ì •ë³´
recipient_info = {
    'GENDER': "ì—¬ì„±",
    'AGE_GROUP': "30ëŒ€",
    'RELATION': "ì—°ì¸",
    'ANNIVERSARY': "100ì¼",
}

# â–¶ï¸ ìƒí™© ì •ë³´ ì´ˆê¸° ìƒíƒœ
situation_info = {
    "closeness": "",
    "emotion": "",
    "preferred_style": "",
    "price_range": ""
}
turn_count = 0

# â–¶ï¸ ê²¬ê³ í•œ JSON íŒŒì‹±
def robust_json_extract(text):
    if '```' in text:
        text = text.split('```')[1].strip()
    try:
        return json.loads(text)
    except:
        try:
            return ast.literal_eval(text)
        except:
            return {}

# â–¶ï¸ ìƒí™© ì •ë³´ ì¶©ì¡± ì¡°ê±´
def is_situation_complete(info):
    required = ["closeness", "emotion", "preferred_style", "price_range"]
    return all(isinstance(info[k], str) and info[k].strip() for k in required)

# â–¶ï¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
system_message = """
<ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸>
ë‹¹ì‹ ì€ ê°„ë‹¨í•œ ì´ë²¤íŠ¸ìš© ì„ ë¬¼ì„ ì¶”ì²œí•´ì£¼ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.
ëŒ€í™” ì´ˆë°˜ì—ëŠ” ì‚¬ìš©ìì—ê²Œ í•„ìš”í•œ ì„ ë¬¼ì˜ ë§¥ë½ì„ ë¬¼ì–´ë³´ê³ , êµ¬ì²´ì ì¸ ì„ ë¬¼ ì œì•ˆì€ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

ë‹¤ìŒ í•­ëª© ì¤‘ emotion, preferred_style, price_range ì´ 3ê°€ì§€ê°€ ëª¨ë‘ ì±„ì›Œì¡Œì„ ë•Œë§Œ ì¶”ì²œì„ ì‹œì‘í•˜ì„¸ìš”.
closenessëŠ” ì„ íƒ í•­ëª©ì…ë‹ˆë‹¤.

ê·¸ ì „ê¹Œì§€ëŠ” ë°˜ë“œì‹œ ì§ˆë¬¸ë§Œ í•˜ë©° ì •ë³´ë¥¼ ìœ ë„í•˜ì„¸ìš”.
ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ êµ¬ì–´ì²´ë¡œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”.
"""

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", system_message),
    MessagesPlaceholder(variable_name="chat_history"),
    HumanMessagePromptTemplate.from_template("{input}")
])
# conversation = RunnableWithMessageHistory(
#     chat_model,
#     chat_prompt,
#     memory=memory
# )

conversation = ConversationChain(llm=chat_model, prompt=chat_prompt, memory=memory, verbose=False)
# â–¶ï¸ ìƒí™© ì •ë³´ ì¶”ë¡  í”„ë¡¬í”„íŠ¸
situation_info_prompt = PromptTemplate(
    input_variables=["chat_history", "current_info"],
    template="""
ì—„ê²©í•œ ì •ë³´ ì¶”ì¶œ ê°€ì´ë“œë¼ì¸:
1. ëŒ€í™” ë‚´ìš©ì— ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ëœ ì •ë³´ë§Œ ì¶”ì¶œ
2. ì¶”ë¡ ì´ë‚˜ ì„ì˜ í•´ì„ ê¸ˆì§€
3. ì–¸ê¸‰ë˜ì§€ ì•Šì€ í•„ë“œëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ìœ ì§€

ëŒ€í™” ë‚´ìš©:
{chat_history}

í˜„ì¬ ìƒí™© ì •ë³´:
{current_info}

ë‹¤ìŒ í•­ëª©ì„ ì¶”ë¡ í•˜ì„¸ìš”: 
ì‚¬ìš©ìì™€ ì„ ë¬¼ ë°›ëŠ” ëŒ€ìƒê³¼ì˜ ì¹œë°€ë„: ì§ˆë¬¸ì„ í†µí•˜ì§€ ì•Šê³  ëŒ€í™” ë‚´ìš©ì„ í†µí•´ ì¶”ë¡ , 
ì„ ë¬¼í•˜ëŠ” ê°ì •, 
ì„ ë¬¼ ë°›ëŠ” ì‚¬ëŒì˜ ì„ í˜¸, 
ì˜ˆì‚°: ì‚¬ìš©ìê°€ ëª…í™•í•˜ê²Œ ì–¸ê¸‰í•œ ë²”ìœ„ë§Œ ê¸°ë¡, ì„ì˜ ì¶”ì • ê¸ˆì§€

ë‚˜ë¨¸ì§€ ë‚´ìš©ì€ ìµœì†Œ 1ë²ˆ ì´ìƒ ì§ˆë¬¸í•´ì•¼ í•©ë‹ˆë‹¤.
ì‚¬ìš©ì ë‹µë³€ì— ìˆëŠ” ë‚´ìš©ë§Œ current_infoì—ì„œ ìˆ˜ì •í•˜ì—¬ ì¶œë ¥í•©ë‹ˆë‹¤.
ì‚¬ìš©ì ë‹µë³€ì´ ëª…í™•í•˜ì§€ ì•Šì€ í•­ëª©ì€ "ì—†ë‹¤", "ëª¨ë¦„" ë“±ì˜ í‘œí˜„ìœ¼ë¡œ ì±„ì›Œë„ ë©ë‹ˆë‹¤.
ì½”ë“œë¸”ëŸ­ ì—†ì´ JSON í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ì¶œë ¥í•˜ì„¸ìš”.
"""
)

# situation_info_chain = chat_model.bind(tags=["situation_info"]).with_fallbacks([chat_model])
situation_info_chain = situation_info_prompt | chat_model

# â–¶ï¸ ê²€ìƒ‰ í‚¤ì›Œë“œ í”„ë¡¬í”„íŠ¸
search_query_prompt = PromptTemplate(
    input_variables=["chat_history", "situation_info"],
    template="""
ë‹¤ìŒ ëŒ€í™” ë‚´ìš©ê³¼ ìƒí™© ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬, ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìƒì„±í•˜ì„¸ìš”.

[ëŒ€í™” ë‚´ìš©]
{chat_history}

[ìƒí™© ì •ë³´]
{situation_info}

â†’ ìƒí™©ì— ê°€ì¥ ì í•©í•œ ìƒí’ˆì„ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ í•µì‹¬ í‚¤ì›Œë“œë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì¶œë ¥í•´ ì£¼ì„¸ìš”.
"""
)

# search_query_chain = LLMChain(llm=chat_model, prompt=search_query_prompt)
search_query_chain = search_query_prompt | chat_model

# â–¶ï¸ ì¶”ì²œ ì´ìœ  í”„ë¡¬í”„íŠ¸
recommend_prompt = PromptTemplate(
    input_variables=["query", "context"],
    template="""
[ì‚¬ìš©ì ìš”ì²­]
{query}

[ì¶”ì²œ ìƒí’ˆ ëª©ë¡]
{context}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë”°ëœ»í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì¶”ì²œ ì´ìœ ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.
"""
)

# rag_response_chain = LLMChain(llm=chat_model, prompt=recommend_prompt)
# recommend_response_chain = recommend_prompt | chat_model

# â–¶ï¸ ìƒí’ˆ í¬ë§·
def format_products(docs):
    return "\n\n".join([
        f"ìƒí’ˆëª…: {doc.metadata.get('title')}\n"
        f"ë¸Œëœë“œ: {doc.metadata.get('brand')}\n"
        f"ê°€ê²©: {doc.metadata.get('price')}\n"
        f"ìƒí’ˆ ë§í¬: {doc.metadata.get('product_url')}"
        for doc in docs
    ])

# â–¶ï¸ ìƒí™© ì •ë³´ ì—…ë°ì´íŠ¸
def update_situation():
    chat_history_str = "\n".join([
        f"{msg.type}: {msg.content}" for msg in memory.chat_memory.messages
    ])
    result = situation_info_chain.invoke({
        "chat_history":chat_history_str,
        "current_info":json.dumps(situation_info, ensure_ascii=False)
    })
    updated = robust_json_extract(result.content)
    if updated:
        for k in situation_info:
            val = updated.get(k, "").strip()
            if val:
                situation_info[k] = val
    else:
        print("[âš ï¸ ìƒí™© ì •ë³´ íŒŒì‹± ì‹¤íŒ¨] ì‘ë‹µ ì›ë¬¸:", result)
    return chat_history_str

# â–¶ï¸ ì‘ë‹µ ìƒì„±
def generate_response(user_input):
    global turn_count
    turn_count += 1

    print(f"[ğŸ‘¤ ì‚¬ìš©ì ì…ë ¥]\n{user_input}\n")
    llm_response = conversation.invoke({"input":user_input})
    chat_history_str = update_situation()
    print(f"[ğŸ“Œ í˜„ì¬ ìƒí™© ì •ë³´]\n{situation_info}")

    if turn_count >= 2:
        if is_situation_complete(situation_info):
            print("\nğŸ¯ ìƒí™© ì •ë³´ê°€ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒí’ˆ ì¶”ì²œì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            query = search_query_chain.invoke({
                "chat_history":chat_history_str,
                "situation_info":json.dumps(situation_info, ensure_ascii=False)
            }).content.strip()
            print(query)
            
            # ì—ì´ì „íŠ¸ë¥¼ í†µí•œ ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„±
            agent_response = agent_executor.invoke({
                "input": f"{user_input}\n\nê²€ìƒ‰ í‚¤ì›Œë“œ: {query}",
                "chat_history": memory.chat_memory.messages
            })
            if agent_response and 'output' in agent_response:
                return f"\nğŸ’¬ ì—ì´ì „íŠ¸ ì‘ë‹µ:\n{agent_response['output']}"
            else:
                return "ì ì ˆí•œ ì¶”ì²œì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            # ì—ì´ì „íŠ¸ ì‘ë‹µ ì²˜ë¦¬
            # if agent_response and 'output' in agent_response:
            #     docs = retriever.invoke(f"query: {query}")
            #     context = format_products(docs)
            #     # reason = recommend_response_chain.invoke({"query":user_input, "context":context})
            #     return f"\nğŸ“¦ ì¶”ì²œ ìƒí’ˆ ëª©ë¡:\n{context}\n\nğŸ’¬ ì—ì´ì „íŠ¸ ì‘ë‹µ:\n{agent_response['output']}"
            #     return f"\nğŸ“¦ ì¶”ì²œ ìƒí’ˆ ëª©ë¡:\n{context}\n\nğŸ ì¶”ì²œ ì´ìœ :\n{reason}\n\nğŸ’¬ ì—ì´ì „íŠ¸ ì‘ë‹µ:\n{agent_response['output']}"
            # else:
            #     return "ì ì ˆí•œ ì¶”ì²œì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    return f"\n[ğŸ’¬ ì±—ë´‡ ì‘ë‹µ]\n{llm_response['response']}"

# â–¶ï¸ ì‹¤í–‰ ë£¨í”„
def chat():
    print("ğŸ ì±—ë´‡ì„ ì‹œì‘í•©ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ 'ì¢…ë£Œ'ë¼ê³  ì…ë ¥í•˜ì„¸ìš”.\n")
    print(f"ì±—ë´‡: {conversation.invoke({'input': f'user: {recipient_info}'})['response']}")

    while True:
        user_input = input("user: ")
        if user_input.strip().lower() == "ì¢…ë£Œ":
            print("ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        response = generate_response(user_input)
        print(response)

# ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
agent_executor = create_agent()

# â–¶ï¸ ì§„ì…ì 
if __name__ == "__main__":
    chat()
