# states.py
import json, ast, re
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor
from langchain.prompts import PromptTemplate

# ExtractAction - extract_aciton()
# aciton ì¶”ì¶œ 
ACTION_EXTRACTION_PROMPT = PromptTemplate(
    input_variables=["chat_history", "recipient_info", "situation_info"],
    template="""
<ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸>
ë‹¹ì‹ ì€ ì„ ë¬¼ ì¶”ì²œ ì±—ë´‡ 'ì„¼í”½'ì˜ íŒë‹¨ ë¡œì§ ì—­í• ì„ ë§¡ê³  ìˆìŠµë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ìœ ì¼í•œ ì„ë¬´ëŠ” ì‚¬ìš©ì ì…ë ¥ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì¤‘ ì–´ë–¤ ëª©ì (action)ì— í•´ë‹¹í•˜ëŠ”ì§€ë¥¼ íŒë‹¨í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
ì‘ë‹µ ë©”ì‹œì§€ë¥¼ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”. ì˜¤ì§ "action" í•˜ë‚˜ë§Œ ê²°ì •í•˜ê³  ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.

[ê°€ëŠ¥í•œ í–‰ë™ ìœ í˜• ë° íŒë‹¨ ê¸°ì¤€]

[1. ask]
ìƒí™© ì •ë³´(emotion, preferred_style, price_range, closeness)ê°€ ë‹¤ ì±„ì›Œì ¸ìˆì§€ ì•Šì„ ë•Œ `ask`ë¡œ íŒë‹¨í•˜ì„¸ìš”.

[2. recommend]
ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš°ì—ë§Œ `recommend`ë¡œ íŒë‹¨í•©ë‹ˆë‹¤:
- ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ì„ ë¬¼ì„ "ì¶”ì²œí•´ë‹¬ë¼", "ì°¾ì•„ì¤˜", "ì•Œë ¤ì¤˜", "ì¶”ì²œë°›ê³  ì‹¶ë‹¤" ë“±ì˜ ì˜ì‚¬ë¥¼ ë°í˜
- ìƒí™© ì •ë³´ê°€ ì–´ëŠ ì •ë„ ì±„ì›Œì ¸ ìˆê³ , ìš”ì²­ì´ ëª¨í˜¸í•˜ì§€ ì•ŠìŒ
â†’ ë‹¨, ì˜ë¯¸ê°€ ë¶ˆë¶„ëª…í•˜ê±°ë‚˜ íŒë‹¨ì´ ì• ë§¤í•œ ê²½ìš°ì—” `refine`ìœ¼ë¡œ ëŒë ¤ì•¼ í•©ë‹ˆë‹¤.

[3. compare]
ì¶”ì²œëœ ì—¬ëŸ¬ ìƒí’ˆ ì¤‘ì—ì„œ ì‚¬ìš©ì ìš”ì²­ì´ "ë¹„êµ íŒë‹¨"ì„ ì›í•˜ëŠ” ê²½ìš°ì— í•´ë‹¹í•©ë‹ˆë‹¤.
ì˜ˆ: "Aë‘ B ì¤‘ì— ë­ê°€ ë” ì¢‹ì•„?", "ë¹„êµí•´ì¤˜", "ì–´ë–¤ ê²Œ ë” ë‚«ì§€?" ë“±

[4. refine]
ì…ë ¥ì´ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì— í•´ë‹¹í•˜ë©´ `refine`ìœ¼ë¡œ ì²˜ë¦¬í•˜ì„¸ìš”:
- ì„ ë¬¼ ì¶”ì²œê³¼ ë¬´ê´€í•œ ì§ˆë¬¸
- ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë¸Œëœë“œ ì–¸ê¸‰ ë˜ëŠ” ì‹œìŠ¤í…œ ì™¸ ìš”ì²­
- ì˜¤íƒ€, ì€ì–´, ìœ í–‰ì–´, ì˜ë¯¸ ë¶ˆëª…í™•í•œ í‘œí˜„ ë“±
â†’ ì´ ê²½ìš° askë¥¼ ì¶œë ¥í•˜ì§€ ë§ê³  ë°˜ë“œì‹œ refine ì²˜ë¦¬ë¡œ ì§€ì •í•˜ì„¸ìš”.

[ì¶œë ¥ í˜•ì‹]
- ë°˜ë“œì‹œ ë‹¤ìŒê³¼ ê°™ì€ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤:
{{
  "action": "ask" | "recommend" | "compare" | "refine"
}}
- ë‹¤ë¥¸ í…ìŠ¤íŠ¸ë‚˜ ì„¤ëª… ì—†ì´ JSON ì˜¤ë¸Œì íŠ¸ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì„¸ìš”.

[ëŒ€í™” ë‚´ì—­]
{chat_history}

[ìˆ˜ë ¹ì¸ ì •ë³´]
{recipient_info}

[ìƒí™© ì •ë³´]
{situation_info}
"""
)


# AskQuestion-conversation()
# ìƒí™© ì •ë³´ ì±„ìš°ê¸° ìœ„í•œ ì§ˆë¬¸ ìƒì„±
# CONVERSATION_PROMPT = """
CONVERSATION_PROMPT = PromptTemplate(
    input_variables=["chat_history", "recipient_info", "situation_info"],
    template="""
<ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸>
ë‹¹ì‹ ì€ ì„ ë¬¼ ì¶”ì²œ ì±—ë´‡, 'ì„¼í”½'ì…ë‹ˆë‹¤.

4ê°€ì§€ ìƒí™© ì •ë³´("emotion", "preferred_style", "price_range", "closeness")ë¥¼ ì±„ìš°ê¸° ìœ„í•œ ì§ˆë¬¸ì„ í•˜ì„¸ìš”.
4ê°€ì§€ ìƒí™© ì •ë³´ê°€ ëª¨ë‘ ìˆì–´ì•¼ ì¶”ì²œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
í•œ ë²ˆì— í•œ ì •ë³´ë§Œ ì§ˆë¬¸í•˜ê³ , í•œ ì •ë³´ëŠ” ìµœëŒ€ 1íšŒë§Œ ì§ˆë¬¸ ê°€ëŠ¥í•©ë‹ˆë‹¤. ìµœëŒ€í•œ ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë§íˆ¬ë¡œ ì§ˆë¬¸í•˜ì„¸ìš”.

- ì‹¤ì œ ì¶”ì²œì€ ë‹¹ì‹ ì´ í•˜ì§€ ì•Šê³ , ì™¸ë¶€ ì‹œìŠ¤í…œ(agent)ì´ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì§ì ‘ ìƒí’ˆ ì´ë¦„ì´ë‚˜ ì¶”ì²œ ë¬¸êµ¬ë¥¼ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.
ë‹¤ìŒì€ ì‚¬ìš©ìì™€ ì±—ë´‡ ê°„ì˜ ëŒ€í™”ì…ë‹ˆë‹¤:
{chat_history}

í˜„ì¬ ì±„ì›Œì§„ ìˆ˜ë ¹ì¸ ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
{recipient_info}

ì±„ì›Œì•¼í•˜ëŠ” ìƒí™© ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
{situation_info}
"""
)

# ExtractSituation - extract_situation()
# ìƒí™© ì •ë³´ ì¶”ì¶œ 
SITUATION_EXTRACTION_PROMPT = """
ì—„ê²©í•œ ì •ë³´ ì¶”ì¶œ ê°€ì´ë“œë¼ì¸:
1. ëŒ€í™” ë‚´ìš©ì— ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ëœ ì •ë³´ë§Œ ì¶”ì¶œ
2. ì¶”ë¡ ì´ë‚˜ ì„ì˜ í•´ì„ ê¸ˆì§€
3. ì–¸ê¸‰ë˜ì§€ ì•Šì€ í•„ë“œëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ìœ ì§€

ëŒ€í™” ë‚´ìš©:
{chat_history}

í˜„ì¬ ìƒí™© ì •ë³´:
{current_info}


ì‚¬ìš©ìì˜ ì‘ë‹µì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì •ë³´ë¥¼ ì¶”ë¡ í•˜ì—¬ ì¶”ì¶œí•˜ì„¸ìš”.
[ì¶”ë¡ í•´ì•¼í•˜ëŠ” ì •ë³´]
"closeness" : ì¹œë°€ë„ (ê°€ê¹Œì›€, ì–´ìƒ‰í•¨, ì¹œí•´ì§€ê³  ì‹¶ìŒ, ì• ë§¤í•¨ ë“±ìœ¼ë¡œ ìš”ì•½)
"emotion" : ì„ ë¬¼ì˜ ë™ê¸°ë‚˜ ë°°ê²½ì´ ëœ ê°ì • ìƒíƒœ
"preferred_style" : í¬ë§í•˜ëŠ” ì„ ë¬¼ì˜ ìŠ¤íƒ€ì¼ (~í•œ ëŠë‚Œ, ~í•œ ìŠ¤íƒ€ì¼ë¡œ ìš”ì•½)
"price_range" : ì˜ˆì‚° ë²”ìœ„ (ì˜ˆ: ìƒê´€ ì—†ìŒ, 7ë§Œì›ëŒ€, 3ë§Œì› ì´í•˜ ë“±ë“±)

[ê·œì¹™]
- ì •ë³´ ì¶”ë¡ ì€ ì‘ë‹µì´ ëª…í™•í•  ë•Œì—ë§Œ ì§„í–‰í•´ì•¼ í•©ë‹ˆë‹¤.
- ì‚¬ìš©ì ë‹µë³€ì— í¬í•¨ëœ ë‚´ìš©ë§Œ current_infoì—ì„œ ìˆ˜ì •í•˜ì—¬ ì¶œë ¥í•©ë‹ˆë‹¤.
- ì½”ë“œë¸”ëŸ­ ì—†ì´ JSON í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ì¶œë ¥í•˜ì„¸ìš”.
"""


# Compare - compare_node()
# ë¹„êµ ìˆ˜í–‰
compare_prompt = PromptTemplate(
    input_variables=["user_input", "chat_history", "situation_info", "recipient_info"],
    template="""
[ì„ ë¬¼ ë¹„êµ - compare]
ì‚¬ìš©ìê°€ ì¶”ì²œëœ ìƒí’ˆì„ ë¹„êµí•´ë‹¬ë¼ê³  ìš”ì²­í•˜ë©´ ì¹œì ˆí•˜ê²Œ ë¹„êµ ì‘ë‹µì„ í•˜ì„¸ìš”. 
(ì‚¬ìš©ì ì„ ë¬¼ ë¹„êµ ìš”ì²­ ì˜ˆì‹œ: ë­ê°€ ë” ì¢‹ì€ì§€ ë¹„êµí•´ì¤˜ / Aë‘ B ì¤‘ì— ë­ê°€ ë” ì¢‹ì„ ê²ƒ ê°™ì•„? / ~ë¥¼ ìƒê°í•˜ë©´ Cê°€ ë” ì¢‹ê² ì§€? ë“±)
ìµœì¢… ê²°ì •ì€ ì‚¬ìš©ìì—ê²Œ ë§¡ê¸°ê³ , ì‚¬ìš©ì ì •ë³´ì— ë”°ë¥¸ ë¹„êµ ì‚¬ìœ ë¥¼ ìƒì„±í•˜ëŠ” ë°ì— ì§‘ì¤‘í•˜ì„¸ìš”. 

[ì…ë ¥ ë‚´ìš©]
{user_input}

[ìƒí™© ì •ë³´]
{situation_info}

[ìˆ˜ë ¹ì¸ ì •ë³´]
{recipient_info}

[ì´ì „ ëŒ€í™” ë‚´ì—­]
{chat_history}
"""
)


# Refine - refine_node()
# ê±°ì ˆ ë©”ì‹œì§€, ì¬ì§ˆë¬¸ ë“±
refine_prompt = PromptTemplate(
    input_variables=["user_input"],
    template="""
[ì…ë ¥í™•ì¸ - refine]
refineì€ askë³´ë‹¤ ìš°ì„ ë©ë‹ˆë‹¤. 
refine ì•¡ì…˜ì€ ì‚¬ìš©ìì˜ "ì…ë ¥ ë¬¸ì¥"ë§Œì„ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”. situation_infoì˜ ë‚´ìš©ì´ë‚˜ ìƒíƒœëŠ” refine ì—¬ë¶€ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤.
ìƒí™© ì •ë³´ê°€ ì±„ì›Œì§„ ì´í›„ë¼ë„, ì‚¬ìš©ìì˜ ì…ë ¥ì´ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì— í•´ë‹¹í•  ê²½ìš° [ask]í•˜ì§€ ë§ê³  ê±°ì ˆ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•˜ì„¸ìš”.
- ì„ ë¬¼ ì¶”ì²œê³¼ ë¬´ê´€í•œ ì‘ë‹µ, ì§ˆë¬¸
- ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë¸Œëœë“œë‚˜ ì˜ëª»ëœ ì •ë³´ë¥¼ ì–¸ê¸‰ (ì˜ˆ: ì„¼í”½ ì „ì ì œí’ˆìœ¼ë¡œ ì°¾ì•„ì¤˜ ë“±)
- ê³¼í•œ ì˜¤íƒ€, ìœ í–‰ì–´, ì€ì–´, ëœ»ì„ ì•Œ ìˆ˜ ì—†ê±°ë‚˜ ë¶€ì •í™•í•œ í‘œí˜„ì´ í¬í•¨ëœ ê²½ìš° (ì˜ˆ: ëŠì¢‹ ì„ ë¬¼ ì¶”ì²œí•´ì¤˜ ë“±)

ê±°ì ˆ ë©”ì‹œì§€ì™€ í•¨ê»˜ ì‚¬ìš©ìê°€ ì •ë³´ë¥¼ ë³´ì™„í•˜ê±°ë‚˜ ë‹¤ì‹œ ì…ë ¥í•  ìˆ˜ ìˆë„ë¡ ì¹œì ˆí•˜ê²Œ ì•ˆë‚´ + ì§ˆë¬¸ì„ í•¨ê»˜ ì¶œë ¥í•˜ì„¸ìš”.

[ì…ë ¥ ë‚´ìš©]
{user_input}
"""
)

# ===================== ğŸ”¹ ê³µí†µ ë„êµ¬ ğŸ”¹ =====================

def robust_json_extract(text: str):
    candidates = re.findall(r'```(?:json)?(.*?)```', text, re.DOTALL)
    if candidates:
        text = candidates[0].strip()
    match = re.search(r'\{.*\}', text.replace("\n", " "), re.DOTALL)
    if match:
        text = match.group()
    else:
        return {}
    try:
        result = json.loads(text)
        return result if isinstance(result, dict) else {}
    except Exception:
        try:
            result = ast.literal_eval(text)
            return result if isinstance(result, dict) else {}
        except Exception:
            return {}

# ===================== ğŸ”¹ ìƒíƒœ ë…¸ë“œ í•¨ìˆ˜ë“¤ ğŸ”¹ =====================

def extract_situation(state, llm=None, prompt_template=None) -> dict:
    try:
        print("\n==== extract_situation ì§„ì… ====")
        chat_str = "\n".join(state["chat_history"][-10:])
        current_info = "\n".join(f"{k}: {v}" for k, v in state["situation_info"].items())
        prompt = prompt_template.format(chat_history=chat_str, current_info=current_info)
        llm_response = llm.invoke(prompt)
        print("\n--- [LLM ì‘ë‹µ ì›ë¬¸] ---")
        print(llm_response)
        llm_text = getattr(llm_response, "content", str(llm_response))
        print(f"[LLM ìµœì¢… í…ìŠ¤íŠ¸ ì‘ë‹µ]: {llm_text}")
        extracted = robust_json_extract(llm_text)
        print("--- [íŒŒì‹± ê²°ê³¼] ---")
        print(extracted)
        print("-----------------------")
        if not isinstance(extracted, dict):
            print(f"[extract_situation] dict ì•„ë‹˜! extracted={extracted}")
            extracted = {}
        for k in state["situation_info"]:
            if extracted.get(k):
                state["situation_info"][k] = extracted[k]
        print("==== extract_situation ì¢…ë£Œ ====\n")
        return state
    except Exception as e:
        print(f"[extract_situation ì „ì²´ ì˜ˆì™¸]: {e}")
        return state

def extract_action(state, llm, prompt_template):
    try:
        chat_history = "\n".join(state.get("chat_history", [])[-10:])
        recipient_info = state.get("recipient_info", {})
        situation_info = state.get("situation_info", {})
        prompt = prompt_template.format(
            chat_history=chat_history,
            recipient_info=recipient_info,
            situation_info=situation_info,
        )
        response = llm.invoke(prompt)
        message = getattr(response, "content", "").strip()
        print("[ExtractAction LLM ì‘ë‹µ]:")
        print(message)
        parsed = robust_json_extract(message)
        print("[Parsed JSON]:", parsed)
        if not isinstance(parsed, dict):
            print("[extract_action ê²½ê³ ] ì˜¬ë°”ë¥´ì§€ ì•Šì€ JSON. ê¸°ë³¸ê°’ 'ask'ë¡œ ì„¤ì •.")
            return {
                **state,
                "action": "ask",
                "output": "ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ ì£¼ì‹¤ ìˆ˜ ìˆì„ê¹Œìš”?"
            }
        action = parsed.get("action", "ask")
        print(f"ğŸ‘‰ ê²°ì •ëœ action: {action}")
        return { **state, "action": action }
    except Exception as e:
        print("[extract_action ì˜ˆì™¸]:", e)
        return {
            **state,
            "action": "ask",
            "output": "ì£„ì†¡í•´ìš”. ë‹¤ì‹œ í•œ ë²ˆ ì…ë ¥í•´ ì£¼ì‹¤ ìˆ˜ ìˆì„ê¹Œìš”?"
        }

def call_agent(state, agent_executor: AgentExecutor = None) -> dict:
    history_str = "\n".join(state.get("chat_history", [])[-10:])
    try:
        user_intent = (
            f"[ì¶”ì¶œëœ ì¡°ê±´]\nê°ì •: {state['situation_info'].get('emotion')}, \n"
            f"ìŠ¤íƒ€ì¼: {state['situation_info'].get('preferred_style')}, \n"
            f"ì˜ˆì‚°: {state['situation_info'].get('price_range')}ì›\n"
            f"ì¹œë°€ë„: {state['situation_info'].get('closeness')}\n"
            f"[ìˆ˜ë ¹ì¸ ì •ë³´]\n{state.get('recipient_info', {})}\n"
            f"[ëŒ€í™” ë§¥ë½]\n{history_str}"
        )

        stream_result = ""
        if agent_executor:
            for chunk in agent_executor.stream({
                "input": user_intent,
                "chat_history": state.get("chat_history", [])
            }):
                value = chunk.get("output") if isinstance(chunk, dict) else str(chunk)
                if value:
                    print(value, end="", flush=True)
                    stream_result += value
            agent_response = stream_result
        else:
            agent_response = "ì—ì´ì „íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."

        return {
            **state,
            "output": agent_response
        }

    except Exception as e:
        print(f"[call_agent ì—ëŸ¬]: {e}")
        return {
            **state,
            "output": "ì¶”ì²œ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        }

def final_response(state) -> dict:
    try:
        return {
            **state,
            "output": state.get("output")
        }
    except Exception as e:
        print(f"[final_response ì—ëŸ¬]: {e}")
        return {
            **state,
            "output": "ìµœì¢… ì‘ë‹µ ìƒì„± ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        }

# ===================== ğŸ”¹ ê³µí†µ ì¶œë ¥ ë…¸ë“œ (stream ê¸°ë°˜) ğŸ”¹ =====================

# def stream_output(state, llm: ChatOpenAI, prompt_template):
#     try:
#         user_input = state["chat_history"][-1]
#         prompt = prompt_template.format(
#             user_input=user_input,
#             chat_history="\n".join(state.get("chat_history", [])[-10:]),
#             recipient_info=state.get("recipient_info", {}),
#             situation_info=state.get("situation_info", {})
#         )
#         output = ""
#         for chunk in llm.stream(prompt):
#             token = getattr(chunk, "content", "")
#             output += token
#             yield token  # ì‹¤ì‹œê°„ ì¶œë ¥
#         yield {
#             **state,
#             "output": output
#         }
#     except Exception as e:
#         print(f"[stream_output ì˜ˆì™¸]: {e}")
#         yield {
#             **state,
#             "output": "ì¶œë ¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
#         }

from langchain_core.messages import AIMessage

# def stream_output(state, llm: ChatOpenAI, prompt_template):
#     print("\n==== stream_output ì§„ì… ====")
#     try:
#         chat_history = state.get("chat_history", [])
#         recipient_info = state.get("recipient_info", {})
#         situation_info = state.get("situation_info", {})

#         input_vars = set(prompt_template.input_variables)

#         if "user_input" in input_vars:
#             if not chat_history:
#                 raise ValueError("[stream_output] chat_historyê°€ ë¹„ì–´ ìˆìŒ")
#             user_input = chat_history[-1]
#             prompt = prompt_template.format(user_input=user_input)
#         else:
#             prompt = prompt_template.format(
#                 chat_history="\n".join(chat_history[-10:]),
#                 recipient_info=recipient_info,
#                 situation_info=situation_info
#             )

#         print("\n--- [LLM ì „ë‹¬ prompt] ---")
#         print(prompt)
#         print("-------------------------")

#         output = ""
#         for chunk in llm.stream(prompt):
#             token = getattr(chunk, "content", "")
#             output += token
#             yield AIMessage(content=token)  # âœ… ì´ë ‡ê²Œ í•˜ë©´ consoleì—ë„ stream ì¶œë ¥ë¨

#         # ë§ˆì§€ë§‰ì— ìƒíƒœ ë°˜í™˜
#         yield {
#             **state,
#             "output": output,
#             "chat_history": chat_history + [output]
#         }

#     except Exception as e:
#         print(f"[stream_output ì˜ˆì™¸]: {e}")
#         yield {
#             **state,
#             "output": "ì¶œë ¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
#         }
def stream_output(state, llm: ChatOpenAI, prompt_template):
    print("\n==== stream_output ì§„ì… ====")
    try:
        chat_history = state.get("chat_history", [])
        recipient_info = state.get("recipient_info", {})
        situation_info = state.get("situation_info", {})

        input_vars = set(prompt_template.input_variables)

        if "user_input" in input_vars:
            if not chat_history:
                raise ValueError("[stream_output] chat_historyê°€ ë¹„ì–´ ìˆìŒ")
            user_input = chat_history[-1]
            prompt = prompt_template.format(user_input=user_input)
        else:
            prompt = prompt_template.format(
                chat_history="\n".join(chat_history[-10:]),
                recipient_info=recipient_info,
                situation_info=situation_info
            )


        output = ""
        for chunk in llm.stream(prompt):
            token = getattr(chunk, "content", "")
            output += token
            # âœ… ì—¬ê¸°ë¥¼ ì—†ì• ê±°ë‚˜ ë¡œê·¸ë¡œë§Œ ì²˜ë¦¬
            print(token, end="", flush=True)

        # ë§ˆì§€ë§‰ì— ìƒíƒœ ë°˜í™˜ (dictë¡œ!)
        yield {
            **state,
            "output": output,
            "chat_history": chat_history + [output]
        }

    except Exception as e:
        print(f"[stream_output ì˜ˆì™¸]: {e}")
        yield {
            **state,
            "output": "ì¶œë ¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        }
